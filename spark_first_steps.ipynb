{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Premiers pas avec Spark\n",
    "\n",
    "Introduction à [Spark](https://spark.apache.org/) et aux [RDD](https://www.cs.cmu.edu/~pavlo/courses/fall2013/static/slides/spark.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"my_id_menu_nb\">run previous cell, wait for 2 seconds</div>\n",
       "<script>\n",
       "function repeat_indent_string(n){\n",
       "    var a = \"\" ;\n",
       "    for ( ; n > 0 ; --n)\n",
       "        a += \"    \";\n",
       "    return a;\n",
       "}\n",
       "// look up into all sections and builds an automated menu //\n",
       "var update_menu_string = function(begin, lfirst, llast, sformat, send, keep_item, begin_format, end_format) {\n",
       "    var anchors = document.getElementsByClassName(\"section\");\n",
       "    if (anchors.length == 0) {\n",
       "        anchors = document.getElementsByClassName(\"text_cell_render rendered_html\");\n",
       "    }\n",
       "    var i,t;\n",
       "    var text_menu = begin;\n",
       "    var text_memo = \"<pre>\\nlength:\" + anchors.length + \"\\n\";\n",
       "    var ind = \"\";\n",
       "    var memo_level = 1;\n",
       "    var href;\n",
       "    var tags = [];\n",
       "    var main_item = 0;\n",
       "    var format_open = 0;\n",
       "    for (i = 0; i <= llast; i++)\n",
       "        tags.push(\"h\" + i);\n",
       "\n",
       "    for (i = 0; i < anchors.length; i++) {\n",
       "        text_memo += \"**\" + anchors[i].id + \"--\\n\";\n",
       "\n",
       "        var child = null;\n",
       "        for(t = 0; t < tags.length; t++) {\n",
       "            var r = anchors[i].getElementsByTagName(tags[t]);\n",
       "            if (r.length > 0) {\n",
       "child = r[0];\n",
       "break;\n",
       "            }\n",
       "        }\n",
       "        if (child == null) {\n",
       "            text_memo += \"null\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        if (anchors[i].hasAttribute(\"id\")) {\n",
       "            // when converted in RST\n",
       "            href = anchors[i].id;\n",
       "            text_memo += \"#1-\" + href;\n",
       "            // passer à child suivant (le chercher)\n",
       "        }\n",
       "        else if (child.hasAttribute(\"id\")) {\n",
       "            // in a notebook\n",
       "            href = child.id;\n",
       "            text_memo += \"#2-\" + href;\n",
       "        }\n",
       "        else {\n",
       "            text_memo += \"#3-\" + \"*\" + \"\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        var title = child.textContent;\n",
       "        var level = parseInt(child.tagName.substring(1,2));\n",
       "\n",
       "        text_memo += \"--\" + level + \"?\" + lfirst + \"--\" + title + \"\\n\";\n",
       "\n",
       "        if ((level < lfirst) || (level > llast)) {\n",
       "            continue ;\n",
       "        }\n",
       "        if (title.endsWith('¶')) {\n",
       "            title = title.substring(0,title.length-1).replace(\"<\", \"&lt;\")\n",
       "         .replace(\">\", \"&gt;\").replace(\"&\", \"&amp;\");\n",
       "        }\n",
       "        if (title.length == 0) {\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        while (level < memo_level) {\n",
       "            text_menu += end_format + \"</ul>\\n\";\n",
       "            format_open -= 1;\n",
       "            memo_level -= 1;\n",
       "        }\n",
       "        if (level == lfirst) {\n",
       "            main_item += 1;\n",
       "        }\n",
       "        if (keep_item != -1 && main_item != keep_item + 1) {\n",
       "            // alert(main_item + \" - \" + level + \" - \" + keep_item);\n",
       "            continue;\n",
       "        }\n",
       "        while (level > memo_level) {\n",
       "            text_menu += \"<ul>\\n\";\n",
       "            memo_level += 1;\n",
       "        }\n",
       "        text_menu += repeat_indent_string(level-2);\n",
       "        text_menu += begin_format + sformat.replace(\"__HREF__\", href).replace(\"__TITLE__\", title);\n",
       "        format_open += 1;\n",
       "    }\n",
       "    while (1 < memo_level) {\n",
       "        text_menu += end_format + \"</ul>\\n\";\n",
       "        memo_level -= 1;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    text_menu += send;\n",
       "    //text_menu += \"\\n\" + text_memo;\n",
       "\n",
       "    while (format_open > 0) {\n",
       "        text_menu += end_format;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    return text_menu;\n",
       "};\n",
       "var update_menu = function() {\n",
       "    var sbegin = \"\";\n",
       "    var sformat = '<a href=\"#__HREF__\">__TITLE__</a>';\n",
       "    var send = \"\";\n",
       "    var begin_format = '<li>';\n",
       "    var end_format = '</li>';\n",
       "    var keep_item = -1;\n",
       "    var text_menu = update_menu_string(sbegin, 2, 4, sformat, send, keep_item,\n",
       "       begin_format, end_format);\n",
       "    var menu = document.getElementById(\"my_id_menu_nb\");\n",
       "    menu.innerHTML=text_menu;\n",
       "};\n",
       "window.setTimeout(update_menu,2000);\n",
       "            </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jyquickhelper import add_notebook_menu\n",
    "add_notebook_menu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Deux ou trois petites choses à ne pas oublier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local et cluster\n",
    "\n",
    "[Spark](http://spark.apache.org/) n'est pas un langage de programmation mais un environnement de calcul distribué. L'installation en locale reproduit ce que [Spark](http://spark.apache.org/) donnerait à grande échelle sur un cluster mais ce n'est pas rigoureusement identique. En particulier cela veut dire que si votre script tourne en local sur un petit jeu de données, il est possible qu'il échoue sur le cluster :\n",
    "\n",
    "* Les dépendances du script sont installées en local mais pas sur chaque machine du cluster [Spark](http://spark.apache.org/). Cela peut se faire à l'installation du cluster pour des dépendances conséquentes ou juste avant l'exécution d'un *job* pour des dépendances ponctuelles.\n",
    "* Les données sur le cluster sont en plus grand nombre, il est fort probable que l'échantillon aléatoire local ne soit pas représentatif.\n",
    "* Les chemins locaux ne fonctionnent pas sur le cluster. Il faudra d'abord uploader les données sur le cluster pour faire tourner le script.\n",
    "* Débugger est compliqué : les print ne marchent pas souvent, surtout si c'est en distribué. Le print va s'exécuter sur une machine distance qui est à mille lieues de votre écran.\n",
    "\n",
    "Quand ça plante sur une machine distante, il faut s'accrocher. Le pire, c'est quand l'erreur arrive pour une observation toute bizarre après cinq heures de calcul. Si le message d'erreur n'est pas trop incompréhensible, on sen tire. En fait, le plus agaçant, c'est quand le calcul est carrément interrompu par le cluster au bout de cinq heures car il décrète que les probabilités d'aboutir sont quasi nulles. Là, on connaît l'erreur (skewed dataset) et on sait qu'on va souffrir pour construire la contournante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark et RDD\n",
    "\n",
    "[Spark](http://spark.apache.org/) ne manipule pas des fichiers mais des [Resilient Distributed Dataset](http://www-bcf.usc.edu/~minlanyu/teach/csci599-fall12/papers/nsdi_spark.pdf) ou *RDD*. En particulier :\n",
    "\n",
    "1. Les *RDD* sont organisés en ligne : ce sont des blocs qui ne seront jamais *cassés* ni *modifié*. Ces lignes ne peuvent pas excéder 2 Go (voir [SPARK-6235](https://issues.apache.org/jira/browse/SPARK-6235)) mais il est conseillé de ne pas aller au-delà de quelques Mo.\n",
    "2. Sauf exception, il est impossible d'accéder à une partie du fichier. Il faut le parcourir en entier (il n'y a pas d'index).\n",
    "3. Les *RDD* fonctionnent comme des *flux* ou *stream*. On peut soit les lire, soit les écrire mais jamais les deux en même temps. Par conséquent, on ne peut pas modifier un *RDD*, il faut toujours en créer un autre.\n",
    "4. Les *RDD* sont distribués. L'ordre des lignes qui le composent n'est pas prévisible.\n",
    "5. Comme l'ordre est imprévisible, on ne stocke **jamais** les noms des colonnes dans les *RDD*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les partitions\n",
    "\n",
    "Il existe une exception au point 2 : les [partitions](http://dev.sortable.com/spark-repartition/). Une partition est un ensemble de lignes traitées par le même processus. La parallélisation ne peut excéder le nombre de partitions. Par défaut, c'est aléatoire (hash hash). Mais on peut tout-à-fait partionner selon une colonne, deux colonnes. D'ailleurs, c'est là-dessus qu'on joue pour optimiser la distribution. Si on réduit (ou grouper) selon une colonne, c'est d'autant plus rapide si le stream est déjà partitionnée sur cette colonne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark et Python\n",
    "\n",
    "Spark est implémenté en Java. L'API [Python](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) permet de faire beaucoup de choses mais :\n",
    "\n",
    "* Elle ne sera jamais aussi complète que l'API [Java](http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/api/java/JavaRDD.html).\n",
    "* Elle sera plus lente que l'API [Java](http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/api/java/JavaRDD.html) ou [Scala](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD) (car [Scala](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD) est une surcouche fonctionnelle de [Java](http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/api/java/JavaRDD.html)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librairies sur Spark\n",
    "\n",
    "Un des succès de Spark est de proposer des extensions dédiées à certains usages comme [MLlib](http://spark.apache.org/mllib/) qui implémente des algorihmes de machine learning distribués, [GraphX](http://spark.apache.org/graphx/) pour des algorithmes sur des graphes. [MLlib](http://spark.apache.org/mllib/) sera bientôt remplacé par ML qui s'appuie sur les [DataFrame](http://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erreur : Cannot run program \"python\"\n",
    "\n",
    "Il vous manque probablement ``PYSPARK_PYTHON``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYSPARK_DRIVER_PYTHON    = anaconda3/bin/jupyter\n",
      "PYSPARK_DRIVER_PYTHON_OPTS= notebook\n",
      "PYSPARK_PYTHON           = anaconda3/bin/python\n",
      "PYSPARK_SUBMIT_ARGS      = \"--name\" \"PySparkShell\" \"pyspark-shell\"\n",
      "SPARK_ENV_LOADED         = 1\n",
      "SPARK_HOME               = <home>/spark-2.2.0-bin-hadoop2.7\n",
      "SPARK_SCALA_VERSION      = 2.10\n",
      "_SPARK_CMD_USAGE         = Usage: ./bin/pyspark [options]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for o, v in sorted(os.environ.items()):\n",
    "    if \"SPARK\" in o.upper():\n",
    "        print(\"{0:25}= {1}\".format(o, v.replace(os.environ[\"HOME\"], \"<home>\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erreur : Output directory  file:/... already exists\n",
    "\n",
    "Spark n'aime pas écrire des données dans un RDD qui existe déjà. Il faut le supprimer. Tout dépend de l'environnement où on se trouve, sur Hadoop ou en local. Comme c'est en local, nous ferons :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fichier.out.txt/_temporary/0', 'dir'),\n",
       " ('fichier.out.txt/_temporary', 'dir'),\n",
       " ('fichier.out.txt', 'dir')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyquickhelper.filehelper import remove_folder\n",
    "def clean(folder):\n",
    "    if os.path.exists(folder):\n",
    "        return remove_folder(folder)\n",
    "    else:\n",
    "        return []\n",
    "clean(\"fichier.out.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vérifier que Spark en local fonctionne\n",
    "\n",
    "On essaye le *\"hello world\"* en *Spark* qui consiste à compter les mots dans un fichier. On prend le fichier du notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o231.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/ismo/Téléchargements/Home/spark_first_steps.ipynb\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:194)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-7365ae6ca328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Home/spark_first_steps.ipynb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fichier.out.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ismo/spark-2.2.0-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduceByKey\u001b[0;34m(self, func, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   1606\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \"\"\"\n\u001b[0;32m-> 1608\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombineByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreduceByKeyLocally\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ismo/spark-2.2.0-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcombineByKey\u001b[0;34m(self, createCombiner, mergeValue, mergeCombiners, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   1832\u001b[0m         \"\"\"\n\u001b[1;32m   1833\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumPartitions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1834\u001b[0;31m             \u001b[0mnumPartitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_defaultReducePartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1836\u001b[0m         \u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ismo/spark-2.2.0-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_defaultReducePartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2242\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2244\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ismo/spark-2.2.0-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2440\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prev_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2442\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ismo/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ismo/spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ismo/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o231.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/ismo/Téléchargements/Home/spark_first_steps.ipynb\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:194)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "text_file = sc.textFile(\"spark_first_steps.ipynb\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "counts.saveAsTextFile(\"fichier.out.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.part-00000.crc',\n",
       " '.part-00001.crc',\n",
       " '._SUCCESS.crc',\n",
       " 'part-00000',\n",
       " 'part-00001',\n",
       " '_SUCCESS']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"fichier.out.txt/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sortie en plusieurs fichiers\n",
    "\n",
    "Un *job* *Spark* est distribué. La sortie d'un *job* *Spark* s'effectue sous la forme de plusieurs stream dans un répertoire, un stream par processus. Cela explique la présence de *part-00000*, *part-00001*. Le fichier ``_SUCCESS`` indique le statut du job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>\n",
       "('', 11686)\n",
       "('[collect](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect)', 1)\n",
       "('SQL](http://spark.apache.org/docs/latest/sql-programming-guide.html)\\\\n\",', 1)\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext pyensae\n",
    "%head fichier.out.txt/part-00000 -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le format dépend du dernier résultat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les opérations de bases\n",
    "\n",
    "Documentation : [programming-guide.html - transformations](http://spark.apache.org/docs/latest/programming-guide.html#transformations).\n",
    "\n",
    "Dans cette section, on considère les données comme un ensemble de lignes de texte. Rien de plus. Donc, pas d'information de type, des conversions quasiment tout le temps. Bref, c'est utile pour comprendre. On y revient quand le reste ne marche pas. En général, on commence par [Spark SQL](http://spark.apache.org/docs/latest/sql-programming-guide.html). Ah oui j'oubliais, on s'en sert beaucoup quand les données ne sont pas structurées et sont décrites par du JSON, genre des logs d'un site internet. Chaque ligne est en fait un gros JSON.\n",
    "\n",
    "On utilise un jeu de données de machine learning [Adult](https://archive.ics.uci.edu/ml/datasets/Adult) légèrement pré-traités et que vous pourrez trouver sur GitHub : [td3a_spark](https://github.com/sdpython/ensae_teaching_cs/tree/master/_doc/notebooks/td3a_spark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"data_adult.txt\"):\n",
    "    from pyquickhelper.filehelper import unzip_files\n",
    "    unzip_files(\"data_adult.zip\", where_to=\".\")\n",
    "assert os.path.exists(\"data_adult.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education_num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital_status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital_gain  capital_loss  hours_per_week  native_country  target  \n",
       "0          2174             0              40   United-States   <=50K  \n",
       "1             0             0              13   United-States   <=50K  \n",
       "2             0             0              40   United-States   <=50K  \n",
       "3             0             0              40   United-States   <=50K  \n",
       "4             0             0              40            Cuba   <=50K  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "df = pandas.read_csv(\"data_adult.txt\", sep=\"\\t\", encoding=\"utf-8\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On enlève le nom des colonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"adult.txt\", sep=\"\\t\", encoding=\"utf-8\", index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>\n",
       "39\t State-gov\t77516\t Bachelors\t13\t Never-married\t Adm-clerical\t Not-in-family\t White\t Male\t2174\t0\t40\t United-States\t <=50K\n",
       "50\t Self-emp-not-inc\t83311\t Bachelors\t13\t Married-civ-spouse\t Exec-managerial\t Husband\t White\t Male\t0\t0\t13\t United-States\t <=50K\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%head adult.txt -n 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### déclaration d'un RDD\n",
    "\n",
    "La déclaration déclare l'existence d'un *RDD* comme on déclare un fichier. Pour l'instant aucune manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"adult.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### enregistrement d'un RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"out\"):\n",
    "    os.mkdir(\"out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clean(\"out/copy_adult.txt\")\n",
    "rdd.saveAsTextFile(os.path.abspath(\"out/copy_adult.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>\n",
       "39\t State-gov\t77516\t Bachelors\t13\t Never-married\t Adm-clerical\t Not-in-family\t White\t Male\t2174\t0\t40\t United-States\t <=50K\n",
       "50\t Self-emp-not-inc\t83311\t Bachelors\t13\t Married-civ-spouse\t Exec-managerial\t Husband\t White\t Male\t0\t0\t13\t United-States\t <=50K\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%head out/copy_adult.txt/part-00000 -n 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lecture locale d'un RDD avec pandas\n",
    "\n",
    "On lit chaque morceaux avant de les concaténer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                  1      2           3   4                    5   \\\n",
       "0  39          State-gov  77516   Bachelors  13        Never-married   \n",
       "1  50   Self-emp-not-inc  83311   Bachelors  13   Married-civ-spouse   \n",
       "\n",
       "                 6               7       8      9     10  11  12  \\\n",
       "0      Adm-clerical   Not-in-family   White   Male  2174   0  40   \n",
       "1   Exec-managerial         Husband   White   Male     0   0  13   \n",
       "\n",
       "               13      14  \n",
       "0   United-States   <=50K  \n",
       "1   United-States   <=50K  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import pandas\n",
    "def read_rdd(path, **options):\n",
    "    pat = os.path.join(path, \"part*\")\n",
    "    all_files = glob.glob(pat)\n",
    "    if len(all_files) == 0:\n",
    "        raise Exception(\"No file to read in '{0}'\".format(path))\n",
    "    merge = []\n",
    "    for f in all_files:\n",
    "        try:\n",
    "            df = pandas.read_csv(f, header=None, **options)\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Unable to read '{0}'\".format(f)) from e\n",
    "        merge.append(df)\n",
    "    if len(merge) == 0:\n",
    "        raise Exception(\"No file to read in '{0}'\".format(path))\n",
    "    concatenated_df = pandas.concat(merge, ignore_index=True)\n",
    "    return concatenated_df\n",
    "\n",
    "data = read_rdd(\"out/copy_adult.txt\", sep=\"\\t\", encoding=\"utf-8\")\n",
    "data.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect\n",
    "\n",
    "Cette opération regroupe les deux précédentes en une seule. Il faut toute de même faire attention de ne pas l'exécuter sur un grand fichier sous peine de faire exploser la mémoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['39\\t State-gov\\t77516\\t Bachelors\\t13\\t Never-married\\t Adm-clerical\\t Not-in-family\\t White\\t Male\\t2174\\t0\\t40\\t United-States\\t <=50K',\n",
       " '50\\t Self-emp-not-inc\\t83311\\t Bachelors\\t13\\t Married-civ-spouse\\t Exec-managerial\\t Husband\\t White\\t Male\\t0\\t0\\t13\\t United-States\\t <=50K']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                  1      2           3   4                    5   \\\n",
       "0  39          State-gov  77516   Bachelors  13        Never-married   \n",
       "1  50   Self-emp-not-inc  83311   Bachelors  13   Married-civ-spouse   \n",
       "\n",
       "                 6               7       8      9     10 11  12  \\\n",
       "0      Adm-clerical   Not-in-family   White   Male  2174  0  40   \n",
       "1   Exec-managerial         Husband   White   Male     0  0  13   \n",
       "\n",
       "               13      14  \n",
       "0   United-States   <=50K  \n",
       "1   United-States   <=50K  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "df = pandas.DataFrame([_.split(\"\\t\") for _ in res])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map\n",
    "\n",
    "Transformer une ligne en une autre ligne. Chaque ligne est traitée indépendemment des autres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['State-gov', 'Bachelors'], ['Self-emp-not-inc', 'Bachelors']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_column(cols, row):\n",
    "    spl = row.split(\"\\t\")\n",
    "    return [spl[i].strip() for i in cols]\n",
    "\n",
    "res = rdd.map(lambda row: extract_column([1,3], row))\n",
    "res.collect()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter\n",
    "\n",
    "Garder ou jeter une ligne. Chaque ligne est traitée indépendemment des autres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['52\\t Self-emp-not-inc\\t209642\\t HS-grad\\t9\\t Married-civ-spouse\\t Exec-managerial\\t Husband\\t White\\t Male\\t0\\t0\\t45\\t United-States\\t >50K',\n",
       " '31\\t Private\\t45781\\t Masters\\t14\\t Never-married\\t Prof-specialty\\t Not-in-family\\t White\\t Female\\t14084\\t0\\t50\\t United-States\\t >50K']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_column(row):\n",
    "    spl = row.split(\"\\t\")\n",
    "    return spl[-1].strip() != \"<=50K\"\n",
    "\n",
    "res = rdd.filter(lambda row: filter_column(row))\n",
    "res.collect()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On combine souvent les deux :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Self-emp-not-inc', 'HS-grad', '>50K'], ['Private', 'Masters', '>50K']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_column_split(row):\n",
    "    return row[-1].strip() != \"<=50K\"\n",
    "\n",
    "res = rdd.map(lambda row: extract_column([1,3,-1], row)) \\\n",
    "         .filter(lambda row: filter_column_split(row))\n",
    "res.collect()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut faire attention aux transformations successives des lignes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMap\n",
    "\n",
    "C'est la principale différence avec SQL. Une ligne peut devenir un nombre variable de lignes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('39',\n",
       "  'State-gov',\n",
       "  '77516',\n",
       "  'Bachelors',\n",
       "  '13',\n",
       "  'Never-married',\n",
       "  'Adm-clerical',\n",
       "  'Not-in-family',\n",
       "  'White',\n",
       "  'Male',\n",
       "  '2174',\n",
       "  '0',\n",
       "  '40',\n",
       "  'United-States',\n",
       "  '<=50K'),\n",
       " ('39',\n",
       "  'State-gov',\n",
       "  '77516',\n",
       "  'Bachelors',\n",
       "  '13',\n",
       "  'Never-married',\n",
       "  'Adm-clerical',\n",
       "  'Not-in-family',\n",
       "  'White',\n",
       "  'Male',\n",
       "  '2174',\n",
       "  '0',\n",
       "  '40',\n",
       "  'United-States',\n",
       "  '<=50K'),\n",
       " ('50',\n",
       "  'Self-emp-not-inc',\n",
       "  '83311',\n",
       "  'Bachelors',\n",
       "  '13',\n",
       "  'Married-civ-spouse',\n",
       "  'Exec-managerial',\n",
       "  'Husband',\n",
       "  'White',\n",
       "  'Male',\n",
       "  '0',\n",
       "  '0',\n",
       "  '13',\n",
       "  'United-States',\n",
       "  '<=50K')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_column_and_multiply_row(n, row):\n",
    "    spl = row.split(\"\\t\")\n",
    "    return [tuple(_.strip() for _ in spl)] * n\n",
    "\n",
    "res = rdd.flatMap(lambda row: extract_column_and_multiply_row(2, row))\n",
    "res.collect()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### group / reduce + mapValues\n",
    "\n",
    "Petite moyenne ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('>50K', (7841, 346963.0)), ('<=50K', (24720, 909294.0))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_age_rich(row):\n",
    "    spl = row.split(\"\\t\")\n",
    "    target = spl[-1].strip()\n",
    "    age = float(spl[0])\n",
    "    return (age, target)\n",
    "\n",
    "def custom_agg(aggset):\n",
    "    temp = list([_[0] for _ in aggset])\n",
    "    return len(temp), sum(temp)\n",
    "\n",
    "ave = rdd.map(extract_age_rich).groupBy(lambda row: row[1]).mapValues(custom_agg)\n",
    "fin = ave.collect()\n",
    "fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sort\n",
    "\n",
    "Je n'en parle pas. Trier un gros jeu de données est à proscrire. On peut trier au sein d'un groupe mais **jamais** un stream entier. Ca fait presque dix ans que j'écris des jobs map/reduce, je n'ai jamais écrit un *sort* sur tout un jeu de données. Ca s'appelle flinguer de la CPU pour rien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join\n",
    "\n",
    "Et on remet la moyenne dans le stream initial. Il vaut mieux regarder la documentation de la méthode [join](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.join) avant de commencer à lire le code qui suit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('>50K',\n",
       "  (['52',\n",
       "    ' Self-emp-not-inc',\n",
       "    '209642',\n",
       "    ' HS-grad',\n",
       "    '9',\n",
       "    ' Married-civ-spouse',\n",
       "    ' Exec-managerial',\n",
       "    ' Husband',\n",
       "    ' White',\n",
       "    ' Male',\n",
       "    '0',\n",
       "    '0',\n",
       "    '45',\n",
       "    ' United-States',\n",
       "    ' >50K'],\n",
       "   (7841, 346963.0))),\n",
       " ('>50K',\n",
       "  (['31',\n",
       "    ' Private',\n",
       "    '45781',\n",
       "    ' Masters',\n",
       "    '14',\n",
       "    ' Never-married',\n",
       "    ' Prof-specialty',\n",
       "    ' Not-in-family',\n",
       "    ' White',\n",
       "    ' Female',\n",
       "    '14084',\n",
       "    '0',\n",
       "    '50',\n",
       "    ' United-States',\n",
       "    ' >50K'],\n",
       "   (7841, 346963.0)))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_key = rdd.map(lambda row: row.split(\"\\t\")).map(lambda row: (row[-1].strip(), row))\n",
    "join = add_key.join(ave)\n",
    "join.collect()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence à comprendre pourquoi [Spark SQL](http://spark.apache.org/docs/latest/sql-programming-guide.html), ça risque d'être pas mal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### le choix existentiel du join : le petit join\n",
    "\n",
    "On fait souvent une opération qui consiste à garder les lignes pour lesquelles une certaine valeur appartient à un ensemble. On peut faire un join classique ou alors l'ensemble est petit, traiter ce join comme un map. On broadcaste l'ensemble à chaque processus exécutant le map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 30, 40]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "ages = sc.broadcast([20, 30, 40])\n",
    "ages.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['30\\t State-gov\\t141297\\t Bachelors\\t13\\t Married-civ-spouse\\t Prof-specialty\\t Husband\\t Asian-Pac-Islander\\t Male\\t0\\t0\\t40\\t India\\t >50K',\n",
       " '40\\t Private\\t121772\\t Assoc-voc\\t11\\t Married-civ-spouse\\t Craft-repair\\t Husband\\t Asian-Pac-Islander\\t Male\\t0\\t0\\t40\\t ?\\t >50K']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = rdd.filter(lambda row: int(row.split(\"\\t\")[0]) in ages.value )\n",
    "subset.collect()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### les trucs qui servent parfois parce que ... à l'usage ça sert\n",
    "\n",
    "Ce que font les méthodes associées aux [RDD](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD), un peu comme les itérateurs, n'est pas toujours intuitif, mais il est à peu près sûr qu'elles vous serviront un jour (peut-être après avoir googlé ou bingé comme des fous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_rdd = sc.parallelize([2, 3, 4])\n",
    "simple_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 1, 2, 3]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_rdd.flatMap(lambda x: range(1, x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[histogram](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.histogram), [groupByKey](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### le truc à retenir\n",
    "\n",
    "collect, collect... qu'est-ce que je voulais dire déjà... Ah oui... Un job map/reduce c'est :\n",
    "\n",
    "1. La déclaration des flux d'entrées.\n",
    "2. Le traitement à proprement parler.\n",
    "3. La déclaration des flux de sorties.\n",
    "\n",
    "A moins d'écrire du java bas niveau, le job est transformé en un plan d'exécution qui n'est jamais exécuté si [collect](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect) ou [save machin chouette](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.saveAsTextFile) n'est jamais exécuté. Bref, c'est du [lazy](https://en.wikipedia.org/wiki/Lazy_evaluation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrame\n",
    "\n",
    "[Spark SQL](http://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "\n",
    "Au début, ça commence par... créer un dataframe. Et comme pour pandas, ces objets retienennt les noms et les types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education_num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "\n",
       "        marital_status        occupation    relationship    race    sex  \\\n",
       "0        Never-married      Adm-clerical   Not-in-family   White   Male   \n",
       "1   Married-civ-spouse   Exec-managerial         Husband   White   Male   \n",
       "\n",
       "   capital_gain  capital_loss  hours_per_week  native_country  target  \n",
       "0          2174             0              40   United-States   <=50K  \n",
       "1             0             0              13   United-States   <=50K  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "data = pandas.read_csv(\"data_adult.txt\", sep=\"\\t\", encoding=\"utf-8\")\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if \"spark\" not in locals():\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.appName(\"nimportequoi\").getOrCreate()  # à ne faire qu'une fois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sdf = spark.createDataFrame(data)  # ça marche\n",
    "sdf = spark.read.csv(\"data_adult.txt\", sep=\"\\t\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+------+-------------+-------------+--------------------+------------------+--------------+-------------------+-------+------------+------------+--------------+--------------+------+\n",
      "|_c0|              _c1|   _c2|          _c3|          _c4|                 _c5|               _c6|           _c7|                _c8|    _c9|        _c10|        _c11|          _c12|          _c13|  _c14|\n",
      "+---+-----------------+------+-------------+-------------+--------------------+------------------+--------------+-------------------+-------+------------+------------+--------------+--------------+------+\n",
      "|age|        workclass|fnlwgt|    education|education_num|      marital_status|        occupation|  relationship|               race|    sex|capital_gain|capital_loss|hours_per_week|native_country|target|\n",
      "| 39|        State-gov| 77516|    Bachelors|           13|       Never-married|      Adm-clerical| Not-in-family|              White|   Male|        2174|           0|            40| United-States| <=50K|\n",
      "| 50| Self-emp-not-inc| 83311|    Bachelors|           13|  Married-civ-spouse|   Exec-managerial|       Husband|              White|   Male|           0|           0|            13| United-States| <=50K|\n",
      "| 38|          Private|215646|      HS-grad|            9|            Divorced| Handlers-cleaners| Not-in-family|              White|   Male|           0|           0|            40| United-States| <=50K|\n",
      "| 53|          Private|234721|         11th|            7|  Married-civ-spouse| Handlers-cleaners|       Husband|              Black|   Male|           0|           0|            40| United-States| <=50K|\n",
      "| 28|          Private|338409|    Bachelors|           13|  Married-civ-spouse|    Prof-specialty|          Wife|              Black| Female|           0|           0|            40|          Cuba| <=50K|\n",
      "| 37|          Private|284582|      Masters|           14|  Married-civ-spouse|   Exec-managerial|          Wife|              White| Female|           0|           0|            40| United-States| <=50K|\n",
      "| 49|          Private|160187|          9th|            5| Married-spouse-a...|     Other-service| Not-in-family|              Black| Female|           0|           0|            16|       Jamaica| <=50K|\n",
      "| 52| Self-emp-not-inc|209642|      HS-grad|            9|  Married-civ-spouse|   Exec-managerial|       Husband|              White|   Male|           0|           0|            45| United-States|  >50K|\n",
      "| 31|          Private| 45781|      Masters|           14|       Never-married|    Prof-specialty| Not-in-family|              White| Female|       14084|           0|            50| United-States|  >50K|\n",
      "| 42|          Private|159449|    Bachelors|           13|  Married-civ-spouse|   Exec-managerial|       Husband|              White|   Male|        5178|           0|            40| United-States|  >50K|\n",
      "| 37|          Private|280464| Some-college|           10|  Married-civ-spouse|   Exec-managerial|       Husband|              Black|   Male|           0|           0|            80| United-States|  >50K|\n",
      "| 30|        State-gov|141297|    Bachelors|           13|  Married-civ-spouse|    Prof-specialty|       Husband| Asian-Pac-Islander|   Male|           0|           0|            40|         India|  >50K|\n",
      "| 23|          Private|122272|    Bachelors|           13|       Never-married|      Adm-clerical|     Own-child|              White| Female|           0|           0|            30| United-States| <=50K|\n",
      "| 32|          Private|205019|   Assoc-acdm|           12|       Never-married|             Sales| Not-in-family|              Black|   Male|           0|           0|            50| United-States| <=50K|\n",
      "| 40|          Private|121772|    Assoc-voc|           11|  Married-civ-spouse|      Craft-repair|       Husband| Asian-Pac-Islander|   Male|           0|           0|            40|             ?|  >50K|\n",
      "| 34|          Private|245487|      7th-8th|            4|  Married-civ-spouse|  Transport-moving|       Husband| Amer-Indian-Eskimo|   Male|           0|           0|            45|        Mexico| <=50K|\n",
      "| 25| Self-emp-not-inc|176756|      HS-grad|            9|       Never-married|   Farming-fishing|     Own-child|              White|   Male|           0|           0|            35| United-States| <=50K|\n",
      "| 32|          Private|186824|      HS-grad|            9|       Never-married| Machine-op-inspct|     Unmarried|              White|   Male|           0|           0|            40| United-States| <=50K|\n",
      "| 38|          Private| 28887|         11th|            7|  Married-civ-spouse|             Sales|       Husband|              White|   Male|           0|           0|            50| United-States| <=50K|\n",
      "+---+-----------------+------+-------------+-------------+--------------------+------------------+--------------+-------------------+-------+------------+------------+--------------+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion à pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sdf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>_c1</th>\n",
       "      <th>_c2</th>\n",
       "      <th>_c3</th>\n",
       "      <th>_c4</th>\n",
       "      <th>_c5</th>\n",
       "      <th>_c6</th>\n",
       "      <th>_c7</th>\n",
       "      <th>_c8</th>\n",
       "      <th>_c9</th>\n",
       "      <th>_c10</th>\n",
       "      <th>_c11</th>\n",
       "      <th>_c12</th>\n",
       "      <th>_c13</th>\n",
       "      <th>_c14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>age</td>\n",
       "      <td>workclass</td>\n",
       "      <td>fnlwgt</td>\n",
       "      <td>education</td>\n",
       "      <td>education_num</td>\n",
       "      <td>marital_status</td>\n",
       "      <td>occupation</td>\n",
       "      <td>relationship</td>\n",
       "      <td>race</td>\n",
       "      <td>sex</td>\n",
       "      <td>capital_gain</td>\n",
       "      <td>capital_loss</td>\n",
       "      <td>hours_per_week</td>\n",
       "      <td>native_country</td>\n",
       "      <td>target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _c0                _c1     _c2         _c3            _c4  \\\n",
       "0  age          workclass  fnlwgt   education  education_num   \n",
       "1   39          State-gov   77516   Bachelors             13   \n",
       "2   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "3   38            Private  215646     HS-grad              9   \n",
       "4   53            Private  234721        11th              7   \n",
       "\n",
       "                   _c5                 _c6             _c7     _c8    _c9  \\\n",
       "0       marital_status          occupation    relationship    race    sex   \n",
       "1        Never-married        Adm-clerical   Not-in-family   White   Male   \n",
       "2   Married-civ-spouse     Exec-managerial         Husband   White   Male   \n",
       "3             Divorced   Handlers-cleaners   Not-in-family   White   Male   \n",
       "4   Married-civ-spouse   Handlers-cleaners         Husband   Black   Male   \n",
       "\n",
       "           _c10          _c11            _c12            _c13    _c14  \n",
       "0  capital_gain  capital_loss  hours_per_week  native_country  target  \n",
       "1          2174             0              40   United-States   <=50K  \n",
       "2             0             0              13   United-States   <=50K  \n",
       "3             0             0              40   United-States   <=50K  \n",
       "4             0             0              40   United-States   <=50K  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retour aux RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[59] at javaToPython at null:-2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récuperer le schéma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(_c0,StringType,true),StructField(_c1,StringType,true),StructField(_c2,StringType,true),StructField(_c3,StringType,true),StructField(_c4,StringType,true),StructField(_c5,StringType,true),StructField(_c6,StringType,true),StructField(_c7,StringType,true),StructField(_c8,StringType,true),StructField(_c9,StringType,true),StructField(_c10,StringType,true),StructField(_c11,StringType,true),StructField(_c12,StringType,true),StructField(_c13,StringType,true),StructField(_c14,StringType,true)))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utiliser pandas pour spécifer le format\n",
    "\n",
    "On utilise pandas sur une partie du stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education_num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "\n",
       "        marital_status        occupation    relationship    race    sex  \\\n",
       "0        Never-married      Adm-clerical   Not-in-family   White   Male   \n",
       "1   Married-civ-spouse   Exec-managerial         Husband   White   Male   \n",
       "\n",
       "   capital_gain  capital_loss  hours_per_week  native_country  target  \n",
       "0          2174             0              40   United-States   <=50K  \n",
       "1             0             0              13   United-States   <=50K  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "df = pandas.read_csv(\"data_adult.txt\", sep=\"\\t\", encoding=\"utf-8\")\n",
    "df.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sdf = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: long (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: long (nullable = true)\n",
      " |-- marital_status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: long (nullable = true)\n",
      " |-- capital_loss: long (nullable = true)\n",
      " |-- hours_per_week: long (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- target: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fullsdf = spark.createDataFrame(sdf.rdd, sdf.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: long (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: long (nullable = true)\n",
      " |-- marital_status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: long (nullable = true)\n",
      " |-- capital_loss: long (nullable = true)\n",
      " |-- hours_per_week: long (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- target: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullsdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enregistrement au format parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fullsdf.write.parquet(\"data_adult.schema.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relecture du format parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsdf = spark.read.parquet(\"data_adult.schema.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: long (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: long (nullable = true)\n",
      " |-- marital_status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: long (nullable = true)\n",
      " |-- capital_loss: long (nullable = true)\n",
      " |-- hours_per_week: long (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- target: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newsdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe Spark VS Dataframe pandas\n",
    "\n",
    "Spark a reproduit la même interface que pandas pour ses dataframes excepté que le résultat n'est pas calculé tant qu'on ne choisit pas de sauvegarder le résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fifty = fullsdf [fullsdf.age > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+------+-------------+-------------+-------------------+------------------+---------------+-------------------+-------+------------+------------+--------------+--------------+------+\n",
      "|age|        workclass|fnlwgt|    education|education_num|     marital_status|        occupation|   relationship|               race|    sex|capital_gain|capital_loss|hours_per_week|native_country|target|\n",
      "+---+-----------------+------+-------------+-------------+-------------------+------------------+---------------+-------------------+-------+------------+------------+--------------+--------------+------+\n",
      "| 53|          Private|234721|         11th|            7| Married-civ-spouse| Handlers-cleaners|        Husband|              Black|   Male|           0|           0|            40| United-States| <=50K|\n",
      "| 52| Self-emp-not-inc|209642|      HS-grad|            9| Married-civ-spouse|   Exec-managerial|        Husband|              White|   Male|           0|           0|            45| United-States|  >50K|\n",
      "| 54|          Private|302146|      HS-grad|            9|          Separated|     Other-service|      Unmarried|              Black| Female|           0|           0|            20| United-States| <=50K|\n",
      "| 59|          Private|109015|      HS-grad|            9|           Divorced|      Tech-support|      Unmarried|              White| Female|           0|           0|            40| United-States| <=50K|\n",
      "| 56|        Local-gov|216851|    Bachelors|           13| Married-civ-spouse|      Tech-support|        Husband|              White|   Male|           0|           0|            40| United-States|  >50K|\n",
      "| 54|                ?|180211| Some-college|           10| Married-civ-spouse|                 ?|        Husband| Asian-Pac-Islander|   Male|           0|           0|            60|         South|  >50K|\n",
      "| 53| Self-emp-not-inc| 88506|    Bachelors|           13| Married-civ-spouse|    Prof-specialty|        Husband|              White|   Male|           0|           0|            40| United-States| <=50K|\n",
      "| 57|      Federal-gov|337895|    Bachelors|           13| Married-civ-spouse|    Prof-specialty|        Husband|              Black|   Male|           0|           0|            40| United-States|  >50K|\n",
      "| 53|          Private|144361|      HS-grad|            9| Married-civ-spouse| Machine-op-inspct|        Husband|              White|   Male|           0|           0|            38| United-States| <=50K|\n",
      "| 53|          Private|169846|      HS-grad|            9| Married-civ-spouse|      Adm-clerical|           Wife|              White| Female|           0|           0|            40| United-States|  >50K|\n",
      "| 79|          Private|124744| Some-college|           10| Married-civ-spouse|    Prof-specialty| Other-relative|              White|   Male|           0|           0|            20| United-States| <=50K|\n",
      "| 67|                ?|212759|         10th|            6| Married-civ-spouse|                 ?|        Husband|              White|   Male|           0|           0|             2| United-States| <=50K|\n",
      "| 52|          Private|276515|    Bachelors|           13| Married-civ-spouse|     Other-service|        Husband|              White|   Male|           0|           0|            40|          Cuba| <=50K|\n",
      "| 59|          Private|159937|      HS-grad|            9| Married-civ-spouse|             Sales|        Husband|              White|   Male|           0|           0|            48| United-States| <=50K|\n",
      "| 53|          Private|346253|      HS-grad|            9|           Divorced|             Sales|      Own-child|              White| Female|           0|           0|            35| United-States| <=50K|\n",
      "| 57|          Private|249977|    Assoc-voc|           11| Married-civ-spouse|    Prof-specialty|        Husband|              White|   Male|           0|           0|            40| United-States| <=50K|\n",
      "| 76|          Private|124191|      Masters|           14| Married-civ-spouse|   Exec-managerial|        Husband|              White|   Male|           0|           0|            40| United-States|  >50K|\n",
      "| 56| Self-emp-not-inc|335605|      HS-grad|            9| Married-civ-spouse|     Other-service|        Husband|              White|   Male|           0|        1887|            50|        Canada|  >50K|\n",
      "| 53|          Private| 95647|          9th|            5| Married-civ-spouse| Handlers-cleaners|        Husband|              White|   Male|           0|           0|            50| United-States| <=50K|\n",
      "| 56|     Self-emp-inc|303090| Some-college|           10| Married-civ-spouse|             Sales|        Husband|              White|   Male|           0|           0|            50| United-States| <=50K|\n",
      "+---+-----------------+------+-------------+-------------+-------------------+------------------+---------------+-------------------+-------+------------+------------+--------------+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fifty.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
